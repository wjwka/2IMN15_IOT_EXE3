{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Data Analysis\n",
    "\n",
    "In this assignment you will experiment with\n",
    "\n",
    "- exploring the data collected at a home and a weather station around the Eindhoven area,\n",
    "- building a predictive model for estimating the amount of electricity produced at the home given a weather forecast.\n",
    "\n",
    "This notebook will guide you through the typical steps that such work would involve. It shows how to build a simple linear model and your task is to improve on it, i.e. train a model that will provide a more accurate prediction than the simple one.\n",
    "\n",
    "#### Goal\n",
    "\n",
    "Your task is to train a model on the time series data containing:\n",
    "\n",
    "- measurements of the amount of electricity produced by the solar panels on the roof of a home in the Eindhoven area\n",
    "- weather measurements around the Eindhoven airport\n",
    "\n",
    "in order to predict the hourly solar panel output given the weather forecast for a particular hour of a particular day.\n",
    "\n",
    "#### Jupyter notebooks\n",
    "\n",
    "For those who are new to jupyter notebooks, a typical notebook contains text cells (like this one) interleaved with code cells (the gray boxes). You can execute a cell by selecting it and pressing Shift+Enter. The expression in the last line in a cell is the output of that cell. Try executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "add(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out more about jupyter notbooks at:\n",
    "\n",
    "- https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/\n",
    "- http://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb\n",
    "- http://nbviewer.jupyter.org/github/jupyter/notebook/tree/master/docs/source/examples/Notebook/\n",
    "\n",
    "#### References\n",
    "\n",
    "The \"Python Data Scinence Handbook\" provides a good overview of using python for data data analysis and you are encouraged to consult it during this assignment:\n",
    "\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "\n",
    "The \"Introduction to Statistical Learning\" [ISLR] book provides a good introduction to machine learning from the statistical perspective:\n",
    "\n",
    "- http://www-bcf.usc.edu/~gareth/ISL/\n",
    "\n",
    "#### Deliverable\n",
    "\n",
    "Throughout this notebook you will find cells starting with **TODO: ...** or `# TODO: ...`. Fill in all these TODO cells. You are encouraged to play with the data and extend this notebook with any interesting insights, beyond the TODOs. At the end, deliver the filled in `assignment.ipynb` file.\n",
    "\n",
    "**Important:** Before delivering your notebook, make sure that the cells in your notebook can be executed in sequence without errors, by executing \"Restart & Run All\" from the \"Kernel\" menu.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries\n",
    "\n",
    "In this assignment we will be using mainly the following libraries:\n",
    "\n",
    "- `pandas` for organizing the data\n",
    "- `numpy` for operating on the data\n",
    "- `matplotlib` for visualizing the data\n",
    "- `sklearn` for training and evaluating a model on the data\n",
    "- other utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "The data resides in two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_energy = pd.read_csv(\"energy_train.csv\")\n",
    "raw_energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_weather = pd.read_csv(\"weather_train.csv\")\n",
    "raw_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analysing data it is important to understand its semantics. In IoT one needs to be extra careful, especially when integrating data from various sources. The very basic information are the units of the measurements, but also the specifications of the sensors gathering the data and the processes that are being monitored. For example, in this case the specification of the solar panel instalation states that it can generate max 42 Jouls per minute.  \n",
    "\n",
    "Assume the following for the energy data:\n",
    "- `seconds` is the time the sample was recorded (UTC)\n",
    "- `total_consumption` is the total amount of electricity that was pulled from the grid (kWh)\n",
    "- `total_production` is the total amount of electricity that was pushed into the grid (kWh)\n",
    "- `solar_production` is the amount of eletricity that was produced since the last sample (Wh).\n",
    "- `total_solar_production` is the total amount of electricity produced by the solar panels (kWh)\n",
    "\n",
    "Assume the following for the weather data:\n",
    "- `time` is the time the sample was recorded (UTC)\n",
    "- `sun` measures whether it was cloudy or sunny at the time when the sample was taken, ranging from 0-3, 0 meaning cloudy and 3 meaning sunny.\n",
    "- `temperature` is the still temperature (C)\n",
    "- `chill` is the temperature with the wind chill effect taken into account (C)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Exploration is usually the first step in any data analysis task. Visualization is an important tool for exploring the data. It gives insights into the structure and semantics of the data and indications for how to clean it.\n",
    "\n",
    "The `matplotlib` library provides a collection of useful plots, such as a line plot, scatter plot, histogram, scatter mattrix, etc. You can find out more about this library at \n",
    "\n",
    "- https://matplotlib.org/users/pyplot_tutorial.html\n",
    "- https://matplotlib.org/devdocs/gallery/\n",
    "\n",
    "The `pandas` library also contains convenient wrappers around the `matplotlib` library for visualizing data frames and series:\n",
    "\n",
    "- https://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "Let's draw a simple plot of the energy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = raw_energy[\"seconds\"]\n",
    "Y = raw_energy[\"total_solar_production\"]\n",
    "plt.plot(X, Y)\n",
    "plt.axis([X.min(), X.max(), Y.min(), Y.max()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: experiment with other data visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "\n",
    "Data cleaning is an important part of any data analysis task. According to the general wisdom, most effort actually is spent on data cleaning. It involves preparing the data for the following steps, e.g. filling in missing values, removing outliers, normalizing the data, etc.\n",
    "\n",
    "In the plot above you will have noticed vertical lines. These lines correspond to 0 values and are likely to be measuring errors (e.g. the solar panel meter has crashed). Such outliers will often negatively impact the accuracy of the predictive model and should be removed.\n",
    "\n",
    "Pandas provides convenient methods for selecting subsets of the data that can be used for removing outliers:\n",
    "\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/03.02-data-indexing-and-selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: remove the samples with production total of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer features\n",
    "\n",
    "The goal of this assignment is to predict the hourly energy output of the solar panels given the weather forecast. This requires to relate the energy samples with the weather samples, basically attaching the energy production label to the weather measurements. However, the *total* energy and weather samples are taken approximately every 10 seconds and 10 minutes, respectively.\n",
    "\n",
    "####  Aggregate per hour\n",
    "\n",
    "We can use pandas grouping functions to aggregate the samples per hour, taking the largest  measurement in that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = \"total_solar_production\"\n",
    "energy = raw_energy\n",
    "times = pd.to_datetime(energy[\"seconds\"], unit='s')\n",
    "\n",
    "energy = energy.groupby([times.dt.year, times.dt.month, times.dt.dayofyear, times.dt.hour])[[col]].agg(np.max)\n",
    "energy.index.names = [\"year\", \"month\", \"day\", \"hour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Note that the hourly aggregation using the maximum works well for columns containing the totals. Does it also work the `solar_production` column? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregation, some cells can contain `Nan` values (e.g. when trying to compute a maximum for an hour during which no samples were recorded). Therefore, after performing such operations one should decide what to do with any Nans. In this case we chose to remove such samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy = energy.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grouping, merging and slicing operations, the index of a data frame might need to be recomputed, to reflect the new ordering of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "energy = energy.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the hourly production\n",
    "\n",
    "We are interested in predicting the solar panel output in a given hour. However, the energy data frame so far have contains only the total solar panel output until the end of that hour. Assuming that the data set contains a sample for every hour, i.e. there are no gaps, the hourly output can be compouted by taking the difference between the previous sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "energy[\"production\"] = energy[col] - energy[col].shift(1)\n",
    "energy = energy.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** after creating the `production` column the Nans are droppend and the index is recomputed. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform a similar aggregation for the `sun` column in the weather data. Note that rather than chosing the maximum, we take the average `sun` value per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = raw_weather\n",
    "times = pd.to_datetime(weather[\"time\"])\n",
    "weather = weather.groupby([times.dt.year, times.dt.month, times.dt.dayofyear, times.dt.hour])[\"sun\"].agg(np.mean)\n",
    "weather.index.names = [\"year\", \"month\", \"day\", \"hour\"]\n",
    "weather = weather.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the data frames\n",
    "\n",
    "Now that both data frames are expressed in terms of hours and the energy data frame contains the hourly solar panel output, we can use pandas to merge them.\n",
    "\n",
    "**Important:** when dealing with times in data from various sources one must be very carefull about the semantics of the time, such as the time-zone or the daylight-saving. Luckily, in this assignment both energy and weather data use UTC time, so no extra preprocessing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(energy, weather).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the engineered features\n",
    "\n",
    "After engineering features it is a good idea to explore the data set again, to see if extra cleaning is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = data[\"production\"].values\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.hist(values, 50);\n",
    "print (min(values), max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small variance of the values and their large range suggests there are some outliers. This can be also visualized in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot(df, col):\n",
    "    x = df[\"day\"]\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(x, df[col], 'bo', markersize=1)\n",
    "\n",
    "plot(data, \"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure suggests that there are indeed outliers in the energy production data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** How do you explain these outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when removing the energy samples containing 0 values in `raw_energy` earlier, it was clear how to define an outlier. In this case, however, one must carefully decide how to define an outlier, to avoid introducing bias. You can read more about outlier detection here:\n",
    "\n",
    "- Section 3.3.3 of \"Introduction to Statistical Learning\"\n",
    "- http://scikit-learn.org/stable/modules/outlier_detection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: How do you define an outlier for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Is your outlier definition \"fair\"? Does it detect outliers that are both too large and too small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot the sun feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Do you see any outliers or interesting patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "For training and evaluating the perfromance of a model we need to split the data into a training and a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data[[\"sun\"]].as_matrix()\n",
    "y = data[\"production\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "After a model is trained we would like to estimate how well it is performing, whether we can actually trust its predictions if it were deployed. Model performance can be estimated using various visualisations and statistics (see Section 3.1.3 of [ISLR])\n",
    "\n",
    "#### Model fit\n",
    "\n",
    "If the feature space is 1 or 2 dimensional, then we can easily plot the model to illustrate how well it fits the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_plot = np.linspace(min(X), max(X), 100)\n",
    "X_plot = x_plot.reshape(-1, 1)\n",
    "y_plot = model.predict(X_plot)\n",
    "\n",
    "dots, = plt.plot(X_test, y_test, 'bo', markersize=2, color=\"red\", label=\"training data\");\n",
    "line, = plt.plot(x_plot, y_plot, linewidth=2, label=\"model\");\n",
    "plt.legend(handles=[dots, line]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the R2 score to assign a number to how well the model fits the data. It estimates how much of the variance in the data is explained by the model, i.e. how well the model fits the data. A score of 1 means the model captures all of the variance, and a score of 0 means teh model does not capture any of the variance.\n",
    "\n",
    "You can read more about the R2 score in Section 3.1.3 of \"Introduction to Statistical Learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residuals\n",
    "\n",
    "We can also plot a histogram of the residuals (or errors), i.e. the deviations of the values predicted by our model from the ground truth values (from the test set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "plt.title(\"Distribution of the residuals\")\n",
    "plt.hist(y_test - y_pred, 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the histogram you will notice that distribution of the residuals is not symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Is the model biased, i.e. does it tend to under- or overestimate the solar panel output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is not biased, or if we are not interested in the bias, then we can compute the mean absolute deviation (MAD) to estimate the expected error of a prediction. E.g. in our case a MAD = 0.345 means that we expect on average our predition to be off by 0.345kWh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the model\n",
    "\n",
    "We have shown here a simple linear model mapping the `sun` feature to the `hourly_production` label. However, this model does not perform very well. The problem could be that the model is underfitting or overfitting the data. You can find more about under- and overfitting in Section 2.1.2, 2.1.3, 6.1 of \"Introduction to Statistical Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Is the model underfitting or overfitting the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could try several things to improve the model.\n",
    "\n",
    "#### Add more features\n",
    "\n",
    "One way of improving the simple linear model could be to extend it with more features. You can gain insight into which features could be relevant by plotting the relationship between the features and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: use the scatter matrix graph to explore the relationship between the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Which features are promising for predicting the solar panel output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Are there other features in the `raw_weather` data that can be used for predicting the solar panel output?\n",
    "\n",
    "Note that using additional features may require aggregating them per hour. The `sun` feature was aggregated taking the maximum value within a given hour, but this may not be suitable for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: train the linear model with the added features and evaluate its perfromance in terms of R2 score and mean absolute deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Did your model improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a more or less flexible model\n",
    "\n",
    "Another way is to add flexibility to the model e.g. by changing it to a polynomial model or a neural network. You can read more about simple extensions of the linear model in Section 3.3.2 and 6.2 of \"Introduction to Statistical Learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: explore other models for predicting the solar panel output and evaluate their perfromance in terms of R2 score and mean absolute deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Select the model you find best. Why did you chose this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the forecasting\n",
    "\n",
    "The `weather_test.csv` file contains the weather data that was held out from `weather_train.csv` used for training the solar panel output predictor. This data simulates the weather forcasts and is used for evaluating how your model would perform if it was deployed.\n",
    "\n",
    "Provide the predictions for the hourly solar panel output for the given weather forecasts, i.e. populate the `production` column in the following data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = pd.read_csv('weather_test.csv')\n",
    "times = pd.to_datetime(prediction[\"time\"])\n",
    "prediction['month'] = times.dt.month\n",
    "prediction['day'] = times.dt.dayofyear\n",
    "prediction['hour'] = times.dt.hour\n",
    "prediction = prediction.drop(prediction[prediction['hour'] == 0].index)\n",
    "prediction = prediction[['month', 'day', 'hour']]\n",
    "prediction = prediction.drop_duplicates().reset_index(drop=True)\n",
    "prediction['production'] = np.nan\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** your model will be evaluated on the held out test set. It is therefore safe to train your *final* model on the entire data set that was provided (rather than the X_train, y_train subsets from the train/test split above), to captures all of the information that is available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill in the production column in the forecast data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For automating the evaluation of your predictions, call the evaluate function that is defined in the `evaluate.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, make sure that the `prediction` data frame contains a `production` column and 805 rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
